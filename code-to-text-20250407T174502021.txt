### FILE TREE (SELECTED)

├─ .upm
|  └─ store.json (2 lines)
├─ static
|  ├─ script.js (244 lines)
|  └─ print_transcript.js (71 lines)
├─ templates
|  └─ index.html (79 lines)
├─ main.py (155 lines)


### FILE CONTENTS

FILE: .upm/store.json
----------------------------------------
{"version":2,"languages":{"python-python3-poetry":{"specfileHash":"2485c2aec725be1c25513e7b6d633f3d","lockfileHash":"2138aa8147f4da8d9cdd0336dda7c66a"}}}


FILE: static/script.js
----------------------------------------
// static/script.js
// This JavaScript File Is Used To Interact With The ChatGPT Real Time Voice Model

document.addEventListener('DOMContentLoaded', function() {
    const startButton = document.getElementById('startButton');
    const stopButton = document.getElementById('stopButton');
    const statusDiv = document.getElementById('status');
    const remoteAudio = document.getElementById('remoteAudio'); // Get the audio element

    let pc; // RTCPeerConnection
    let dc; // RTCDataChannel
    let localStream; // User's microphone stream

    startButton.addEventListener('click', startChat);
    stopButton.addEventListener('click', stopChat);

    async function startChat() {
        console.log('Attempting to start chat...');
        statusDiv.textContent = 'Connecting...';
        startButton.disabled = true;
        stopButton.disabled = false;

        try {
            // 1. Get an ephemeral key from your server
            const tokenResponse = await fetch("/session");
            if (!tokenResponse.ok) {
                 const errorData = await tokenResponse.json();
                 throw new Error(`Failed to get session token: ${tokenResponse.status} ${tokenResponse.statusText} - ${JSON.stringify(errorData.details || errorData.error)}`);
            }
            const sessionData = await tokenResponse.json();
            const EPHEMERAL_KEY = sessionData.client_secret?.value; // Use optional chaining
            // Use the model specified in the session response or fallback if needed
            const REALTIME_MODEL = sessionData.model || "gpt-4o-realtime-preview";

            if (!EPHEMERAL_KEY) {
                 throw new Error("Ephemeral key (client_secret.value) not found in session response.");
            }
            console.log(`Using model: ${REALTIME_MODEL}`);
            console.log('Ephemeral key received.');

            // 2. Create a peer connection
            pc = new RTCPeerConnection();

            // 3. Set up to play remote audio from the model
            pc.ontrack = e => {
                console.log('Remote track received:', e.track);
                if (e.streams && e.streams[0]) {
                    remoteAudio.srcObject = e.streams[0];
                    console.log('Assigned remote stream to audio element.');
                     // Attempt to play audio programmatically after user interaction
                     remoteAudio.play().catch(e => console.error("Audio play failed:", e));
                } else {
                     console.warn("Received track event without streams.");
                     // Fallback for older browser compatibility if needed, but less common now
                }
            };

            // Handle connection state changes for debugging/status updates
             pc.onconnectionstatechange = event => {
                  statusDiv.textContent = `Connection State: ${pc.connectionState}`;
                  console.log(`Peer Connection State: ${pc.connectionState}`);
                  if (pc.connectionState === 'failed' || pc.connectionState === 'disconnected' || pc.connectionState === 'closed') {
                      stopChat(); // Clean up if connection drops
                  }
             };

             pc.onicecandidateerror = event => {
                 console.error("ICE Candidate Error:", event);
                 statusDiv.textContent = `Error: ICE Candidate Error - ${event.errorCode}: ${event.errorText}`;
             };

             pc.onicegatheringstatechange = () => console.log(`ICE Gathering State: ${pc.iceGatheringState}`);

            // 4. Add local audio track for microphone input in the browser
            console.log('Requesting microphone access...');
            localStream = await navigator.mediaDevices.getUserMedia({
                audio: true
            });
            localStream.getTracks().forEach(track => {
                 pc.addTrack(track, localStream);
                 console.log('Local audio track added.');
             });

            // 5. Set up data channel for sending and receiving events
            dc = pc.createDataChannel("oai-events");
            console.log('Data channel created.');

            dc.onopen = () => {
                 console.log('Data channel opened.');
                 statusDiv.textContent = 'Data channel open. Ready to chat!';
                 // Example: Send initial configuration or greeting if needed
                 // dc.send(JSON.stringify({ type: 'configure', settings: {} }));
             };

            // =============================================================
            // === VVVVVVVVVVVV MODIFICATION START VVVVVVVVVVVV ===
            // =============================================================
            dc.onmessage = (event) => {
                // Log the raw message for debugging
                // console.log('Raw Data channel message received:', event.data);

                try {
                    const messageData = JSON.parse(event.data);

                    // Log the parsed message type for easier debugging
                    // console.log(`Parsed message type: ${messageData.type}`);

                    // Check for the specific event that contains the final transcript for an utterance
                    // Based on OpenAI docs and your transcript_seen.md, 'response.audio_transcript.done' is key.
                    if (messageData.type === "response.audio_transcript.done" && messageData.transcript) {
                        console.log("Received final transcript:", messageData.transcript);
                        // Check if the display function from print_transcript.js exists
                        if (typeof displayFinalTranscript === 'function') {
                            // Call the function to add the transcript to the page
                            displayFinalTranscript(messageData.transcript);
                        } else {
                            console.error("displayFinalTranscript function is not defined. Ensure print_transcript.js is loaded correctly before script.js tries to use it.");
                        }
                    }
                     // Add handlers for other potentially useful events if needed
                     else if (messageData.type === "error") {
                         console.error("Received error event from Realtime API:", messageData);
                         statusDiv.textContent = `Error: ${messageData.message || 'Unknown API error'}`;
                     }
                     else if (messageData.type === "session.created") {
                         console.log("Session created event received:", messageData.session);
                     }
                     // Add more 'else if' blocks here to handle other event types
                     // (e.g., 'input_audio_buffer.speech_started', 'response.created', etc.)
                     // if you need to react to them in the UI or logic.

                } catch (e) {
                    console.error("Failed to parse data channel message or handle event:", e);
                    console.error("Raw data received:", event.data); // Log raw data on error
                }
            };
            // =============================================================
            // === ^^^^^^^^^^^^ MODIFICATION END ^^^^^^^^^^^^ ===
            // =============================================================


             dc.onclose = () => {
                 console.log('Data channel closed.');
                 // Only update status if the connection isn't already closing/closed
                 if (pc && pc.connectionState !== 'closed' && pc.connectionState !== 'disconnected') {
                    statusDiv.textContent = 'Data channel closed.';
                 }
             };

             dc.onerror = (error) => {
                 console.error('Data channel error:', error);
                 statusDiv.textContent = `Error: Data channel error - ${error.message || 'Unknown error'}`;
             };

            // 6. Start the session using the Session Description Protocol (SDP)
            console.log('Creating SDP offer...');
            const offer = await pc.createOffer();
            await pc.setLocalDescription(offer);
            console.log('Local description set.');

            const baseUrl = "https://api.openai.com/v1/realtime";
            // Use the model name obtained from the session endpoint
            const sdpResponse = await fetch(`${baseUrl}?model=${REALTIME_MODEL}`, {
                method: "POST",
                body: offer.sdp, // Send the SDP offer text directly
                headers: {
                    Authorization: `Bearer ${EPHEMERAL_KEY}`,
                    "Content-Type": "application/sdp" // Crucial header
                },
            });

            if (!sdpResponse.ok) {
                const errorText = await sdpResponse.text();
                 throw new Error(`SDP exchange failed: ${sdpResponse.status} ${sdpResponse.statusText} - ${errorText}`);
            }

            const answerSdp = await sdpResponse.text();
            console.log('Received SDP answer.');
            const answer = {
                type: "answer",
                sdp: answerSdp,
            };
            await pc.setRemoteDescription(answer);
            console.log('Remote description set. WebRTC setup complete.');
            // Status update moved to dc.onopen for better user feedback timing
            // statusDiv.textContent = 'Connected!';

        } catch (error) {
            console.error('Error starting chat:', error);
            statusDiv.textContent = `Error: ${error.message}`;
            stopChat(); // Clean up on error
            startButton.disabled = false;
            stopButton.disabled = true;
        }
    }

    function stopChat() {
        console.log('Stopping chat...');
        if (dc) {
            // dc.onclose will handle status update unless connection already closed
            dc.close();
            dc = null; // Nullify after closing
            console.log('Data channel close initiated.');
        }
        if (pc) {
             // Stop sending tracks
             pc.getSenders().forEach(sender => {
                 if (sender.track) {
                     sender.track.stop();
                 }
             });
              // Stop receiving tracks (though srcObject = null is often sufficient)
              pc.getReceivers().forEach(receiver => {
                  if (receiver.track) {
                      receiver.track.stop();
                  }
              });

            // pc.onconnectionstatechange handles status update
            pc.close();
            pc = null; // Nullify after closing
            console.log('Peer connection close initiated.');
        }
        if (localStream) {
            localStream.getTracks().forEach(track => track.stop());
            localStream = null;
            console.log('Local microphone stream stopped.');
        }
        // Clear the audio source
         if (remoteAudio) {
            remoteAudio.srcObject = null;
            remoteAudio.pause(); // Ensure it's paused
            remoteAudio.load(); // Reset the element
         }

        // Update status definitively if not already handled by events
        if (statusDiv.textContent !== 'Disconnected') {
             statusDiv.textContent = 'Disconnected';
        }
        startButton.disabled = false;
        stopButton.disabled = true;
        console.log('Chat stopped and resources released.');
    }
});

FILE: static/print_transcript.js
----------------------------------------
// This JavaScript file will be used to display the transcript of what ChatGPT voice agent is saying on the page as it is said by the voice bot.

// static/print_transcript.js
// This JavaScript file is used to display the transcript of what the voice agent is saying on the page.

// Flag to track if it's the first transcript entry, used to remove the initial placeholder text.
let isFirstTranscript = true;

/**
 * Displays a complete transcript utterance in the transcript output area on the HTML page.
 * It appends the new text as a paragraph and scrolls the container to the bottom.
 *
 * @param {string} text - The transcript text received from the assistant (usually a complete sentence or phrase).
 */
function displayFinalTranscript(text) {
    // Get the HTML element where the transcript should be displayed.
    const transcriptContainer = document.getElementById('transcriptOutput');

    // Safety check: Make sure the transcript container element exists in the DOM.
    if (!transcriptContainer) {
        console.error("Error: Transcript container element with ID 'transcriptOutput' not found.");
        return; // Exit the function if the container isn't found.
    }

    // If this is the very first transcript being added, find and remove the placeholder text.
    if (isFirstTranscript) {
        const placeholder = transcriptContainer.querySelector('.text-gray-400.italic');
        if (placeholder) {
            transcriptContainer.removeChild(placeholder);
        }
        // Set the flag to false so the placeholder isn't removed again.
        isFirstTranscript = false;
    }

    // Create a new paragraph element (<p>) to hold this line of the transcript.
    const newEntry = document.createElement('p');

    // Add the actual transcript text content to the paragraph.
    // Prepending "Assistant: " makes it clear who is speaking.
    newEntry.textContent = 'Assistant: ' + text;

    // Optional: Add any specific CSS classes if needed for styling the entry.
    // Tailwind classes defined on the container (like font-mono, text-sm) will apply.
    // You could add classes like 'mb-1' here if the container's 'space-y-2' isn't sufficient.
    // newEntry.classList.add('mb-1');

    // Append the newly created paragraph element to the transcript container.
    transcriptContainer.appendChild(newEntry);

    // Automatically scroll the transcript container to the bottom.
    // This ensures the latest transcript entry is always visible.
    transcriptContainer.scrollTop = transcriptContainer.scrollHeight;
}

// Note: This script assumes that your main script ('script.js') will call
// the `displayFinalTranscript(text)` function when it receives the appropriate
// message (e.g., 'response.audio_transcript.done') from the WebSocket/WebRTC data channel.
// Make sure the call in `script.js` passes the relevant transcript string.

// Example of how you might call this from script.js (within the dc.onmessage handler):
/*
   const messageData = JSON.parse(event.data);
   // Check for events containing the final transcript for an utterance
   if (messageData.type === "response.audio_transcript.done" && messageData.transcript) {
       displayFinalTranscript(messageData.transcript);
   } else if (messageData.type === "response.done" && messageData.response?.output?.[0]?.content?.[0]?.transcript) {
       // Fallback check in response.done event, structure might vary
       displayFinalTranscript(messageData.response.output[0].content[0].transcript);
   }
   // Add other event handling as needed...
*/

FILE: templates/index.html
----------------------------------------
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Independence Blue Cross Voice Assistant Demo</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="{{ url_for('static', filename='script.js') }}" defer></script>
    <script src="{{ url_for('static', filename='print_transcript.js') }}" defer></script>
    <style>
        /* Optional: You could define the specific IBX blue here if Tailwind's default blues aren't exact */
        /* .text-ibx-blue { color: #007AC3; } */
        /* .bg-ibx-blue { background-color: #007AC3; } */
        /* For this example, we'll use Tailwind's sky-600 which is a similar shade */

        /* Custom scrollbar styling (optional, webkit specific) */
        #transcriptOutput::-webkit-scrollbar {
            width: 8px;
        }
        #transcriptOutput::-webkit-scrollbar-track {
            background: #f1f1f1; /* Light grey track */
            border-radius: 10px;
        }
        #transcriptOutput::-webkit-scrollbar-thumb {
            background: #888; /* Darker grey thumb */
            border-radius: 10px;
        }
        #transcriptOutput::-webkit-scrollbar-thumb:hover {
            background: #555; /* Darker thumb on hover */
        }
    </style>
</head>
<body class="bg-white text-gray-800 min-h-screen flex flex-col items-center justify-center p-4">

    <div class="bg-white rounded-lg shadow-xl p-6 md:p-8 max-w-4xl w-full border border-gray-200">

        <h1 class="text-2xl md:text-3xl font-bold text-center text-sky-600 mb-6">
            Chat with the Independence Blue Cross Voice Assistant
        </h1>

        <div class="grid grid-cols-1 md:grid-cols-2 gap-6 md:gap-8">

            <div class="flex flex-col space-y-4">
                <p class="text-gray-600 text-sm md:text-base text-center md:text-left">
                    Click "Start Chat" to connect with the Independence Blue Cross voice assistant using your microphone. You can ask questions about the 2025 Individual and Family plans.
                </p>

                <div class="flex justify-center md:justify-start space-x-4">
                    <button id="startButton" class="bg-sky-600 hover:bg-sky-700 text-white font-semibold py-2 px-5 md:px-6 rounded-lg transition duration-200 ease-in-out disabled:opacity-50 disabled:cursor-not-allowed text-sm md:text-base">
                        Start Chat
                    </button>
                    <button id="stopButton" class="bg-red-500 hover:bg-red-600 text-white font-semibold py-2 px-5 md:px-6 rounded-lg transition duration-200 ease-in-out disabled:opacity-50 disabled:cursor-not-allowed text-sm md:text-base" disabled>
                        Stop Chat
                    </button>
                </div>

                <div class="p-3 md:p-4 bg-gray-100 rounded border border-gray-300 flex-grow">
                    <p class="text-xs md:text-sm font-mono text-gray-700">Status:</p>
                    <div id="status" class="text-xs md:text-sm font-mono text-sky-700 break-words">Idle</div>
                </div>

                <audio id="remoteAudio" autoplay playsinline class="w-full hidden"></audio>

                <div class="text-center md:text-left text-xs text-gray-500 pt-2">
                    Ensure your microphone is enabled and allowed by the browser.
                </div>
            </div>

            <div class="flex flex-col">
                 <h2 class="text-xl font-semibold text-center md:text-left text-sky-600 mb-3">
                    Transcript
                 </h2>
                 <div id="transcriptOutput" class="bg-gray-50 border border-gray-300 rounded p-3 h-64 md:h-auto md:flex-grow overflow-y-auto text-sm font-mono text-gray-700 space-y-2">
                     <p class="text-gray-400 italic">Waiting for conversation...</p>
                 </div>
            </div>

        </div> </div> </body>
</html>

FILE: main.py
----------------------------------------
import os
import requests
import json # Added for JSON loading in error handling
from flask import Flask, render_template, jsonify, request

# --- Configuration ---
# It's highly recommended to load sensitive keys from environment variables
# Ensure OPENAI_API_KEY is set in your environment
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
# Use the latest recommended model or the specific one you need
# Update model string as per OpenAI documentation or announcements
# Check OpenAI docs for the current recommended preview model.
OPENAI_REALTIME_MODEL = os.environ.get("OPENAI_REALTIME_MODEL", "gpt-4o-realtime-preview") # Using a reasonable default
OPENAI_SESSION_URL = "https://api.openai.com/v1/realtime/sessions"

# <<< START MODIFICATION: Update filename for instructions >>>
INSTRUCTIONS_FILENAME = "call_center_guide.md" # Define the filename for instructions
# <<< END MODIFICATION >>>


# --- Flask App Initialization ---
app = Flask(__name__, static_url_path='/static')

# --- Helper Function to Read Instructions File ---
# <<< START MODIFICATION: Update function/comments to be more generic >>>
def read_instructions_from_file():
    """Reads the instructions content from the specified file."""
    try:
        # Get the directory of the current script to reliably find the instructions file
        script_dir = os.path.dirname(os.path.abspath(__file__))
        instructions_file_path = os.path.join(script_dir, INSTRUCTIONS_FILENAME)

        print(f"Attempting to read instructions file: {instructions_file_path}") # Log path

        with open(instructions_file_path, 'r', encoding='utf-8') as f: # Specify encoding
            content = f.read().strip()
            print(f"Successfully read instructions from '{INSTRUCTIONS_FILENAME}'.")
            return content

    except FileNotFoundError:
        print(f"Warning: Instructions file '{INSTRUCTIONS_FILENAME}' not found at {instructions_file_path}. Proceeding without custom instructions.")
        return None # Return None if file not found
    except Exception as e:
        print(f"Error reading instructions file '{INSTRUCTIONS_FILENAME}': {e}")
        return None # Return None on other errors
# <<< END MODIFICATION >>>

# --- Routes ---
@app.route('/')
def index():
    """Serves the main HTML page."""
    return render_template('index.html')

@app.route('/session', methods=['GET'])
def get_session_token():
    """
    Server-side endpoint to securely generate an ephemeral OpenAI API key (token)
    and includes the instructions from the guide file during session creation.
    The client-side JavaScript will call this endpoint.
    """
    if not OPENAI_API_KEY:
        return jsonify({"error": "OpenAI API key not configured on the server."}), 500

    # Read the instructions from the specified file before making the API call
    # <<< START MODIFICATION: Call updated function name >>>
    instructions_text = read_instructions_from_file() # Can be None if file not found/error
    # <<< END MODIFICATION >>>


    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json",
    }

    # Base payload for the session creation
    payload = {
        "model": OPENAI_REALTIME_MODEL,
        "voice": "alloy",
        # "modalities": ["audio", "text"], # Optional: Uncomment/adjust if needed

        # --- Optional: Input Transcription settings ---
        # Verify the exact parameter name and structure from API docs if you need this.
        # "input_audio_transcription": {
        #     "model": "whisper-1",
        #     # "language": "en"
        # },

        # --- CORRECT Turn Detection Settings ---
        "turn_detection": {          # <<< The key MUST be exactly "turn_detection"
            "type": "semantic_vad",  # Value: type set to semantic_vad
            "eagerness": "high"      # Value: eagerness set to high
            # 'create_response': True, # Default, usually no need to specify
            # 'interrupt_response': True # Default, usually no need to specify
        }
        # Add any other required top-level parameters for the specific API endpoint here
    }

    # Only add 'instructions' to the payload if the content was read successfully
    # <<< START MODIFICATION: Use updated variable name >>>
    if instructions_text:
        payload["instructions"] = instructions_text
        print(f"Including instructions from '{INSTRUCTIONS_FILENAME}' in session creation request.")
    else:
         print(f"No instructions file '{INSTRUCTIONS_FILENAME}' found or read error; session will be created without custom instructions.")
    # <<< END MODIFICATION >>>


    try:
        print(f"Requesting session from OpenAI with payload: {json.dumps(payload)}") # Log the payload being sent
        response = requests.post(OPENAI_SESSION_URL, headers=headers, json=payload)
        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)

        session_data = response.json()

        # The ephemeral key is within 'client_secret'
        if "client_secret" not in session_data:
            print(f"Error: 'client_secret' not found in OpenAI response: {session_data}")
            return jsonify({"error": "Failed to retrieve client_secret from OpenAI session.", "details": session_data}), 500

        # Log successful session creation with the actual session ID from the response
        print(f"Successfully created OpenAI session ID: {session_data.get('id', 'N/A')}")
        # We don't need to send the instructions text back to the client anymore
        # as it's handled during session creation.
        return jsonify(session_data)

    except requests.exceptions.RequestException as e:
        # Log the error for debugging on the server
        print(f"Error requesting OpenAI session token: {e}")
        # Provide a generic error message to the client
        error_details = str(e)
        if e.response is not None:
            try:
                # Attempt to get JSON error details from OpenAI response
                error_details = e.response.json()
            except json.JSONDecodeError:
                # Fallback to text if response is not JSON
                error_details = e.response.text
        print(f"Error details from OpenAI API: {error_details}")
        return jsonify({"error": "Failed to communicate with OpenAI API.", "details": error_details}), 502 # Bad Gateway might be appropriate
    except Exception as e:
        # Catch any other unexpected errors during the process
        print(f"An unexpected error occurred in /session endpoint: {e}")
        return jsonify({"error": "An internal server error occurred."}), 500

# --- Run the App ---
if __name__ == '__main__':
    # Note: Use host='0.0.0.0' for accessibility within Docker/Replit,
    # but be mindful of security implications in production environments.
    # Use environment variable for port if available, default to 8080 or 5000
    port = int(os.environ.get('PORT', 8080)) # Using 8080 as a common alternative
    # Debug mode should ideally be off in production
    # Read FLASK_DEBUG env var, default to 'false' if not set
    debug_mode = os.environ.get('FLASK_DEBUG', 'False').lower() in ('true', '1', 't')
    print(f"Starting Flask app on host 0.0.0.0 port {port} with debug mode: {debug_mode}")
    app.run(host='0.0.0.0', port=port, debug=debug_mode)

